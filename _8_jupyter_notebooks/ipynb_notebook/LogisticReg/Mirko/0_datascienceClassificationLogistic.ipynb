{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "individual-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_version 3.7.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-christmas",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %CLASS% StockClass: used to create the stock dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "split-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockClass(object):\n",
    "\n",
    "    # Initialization of the StockClass object with the ticker symbol which is use to construct a yf.Ticker object\n",
    "    def __init__(self, ticker, isin=None, exchangeid=None, sector=None, industry=None, country=None, pe=None, eps=None,\n",
    "                 insiderown=None, shsout=None, shsfloat=None, mktcap=None, income=None, sales=None,\n",
    "                 booksh=None, pb=None, roa=None, tp=None, roe=None, roi=None, employees=None, debteq=None, \n",
    "                 groupby=None, confusion_matrix=None, accuracy_report=None, \n",
    "                 confusion_matrix2=None, accuracy_report2=None):\n",
    "        self.name = ticker\n",
    "        self.isin = isin\n",
    "        self.exchangeid = exchangeid\n",
    "        # self.history is the method of the SotckClass object to store data in DataFrame format\n",
    "        self.history = pd.DataFrame\n",
    "        self.investing = pd.DataFrame\n",
    "        self.pickle = pd.DataFrame\n",
    "        self.not_found = np.array([['date', 'ticker']])\n",
    "        self.nanDiv = False\n",
    "        self.nanSplit = False\n",
    "        self.sector = None\n",
    "        self.industry = industry\n",
    "        self.country = country\n",
    "        self.pe = pe\n",
    "        self.eps = eps\n",
    "        self.insiderown = insiderown\n",
    "        self.shsout = shsout\n",
    "        self.shsfloat = shsfloat\n",
    "        self.mktcap = mktcap\n",
    "        self.income = income\n",
    "        self.sales = sales\n",
    "        self.bookh = booksh\n",
    "        self.pb = pb\n",
    "        self.roa = roa\n",
    "        self.tp = tp\n",
    "        self.roe = roe\n",
    "        self.roi = roi\n",
    "        self.employees = employees\n",
    "        self.debteq = debteq\n",
    "        self.rsi = pd.DataFrame\n",
    "        self.groupby = groupby\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.accuracy_report = accuracy_report\n",
    "        self.confusion_matrix2 = confusion_matrix2\n",
    "        self.accuracy_report2 = accuracy_report2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-supply",
   "metadata": {},
   "source": [
    "# 1. First part: import data, cleaning and arranging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-butterfly",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Main packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frozen-upper",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-valley",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "funky-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_df_original = pd.read_csv('open_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "high_df_original = pd.read_csv('high_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "low_df_original = pd.read_csv('low_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "adjclose_df_original = pd.read_csv('adjclose_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "volume_df_original = pd.read_csv('volume_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "ticker_list = list(adjclose_df_original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collected-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open shape before:  (249, 4635)\n",
      "open shape after:  (249, 4632)\n",
      "high shape before:  (249, 4635)\n",
      "high shape after:  (249, 4632)\n",
      "low shape before:  (249, 4635)\n",
      "low shape after:  (249, 4632)\n",
      "high shape before:  (249, 4635)\n",
      "high shape after:  (249, 4632)\n",
      "high shape before:  (249, 4635)\n",
      "high shape after:  (249, 4632)\n"
     ]
    }
   ],
   "source": [
    "# to drop columns with the end of the series NaN (probably not quoted anymore)\n",
    "if open_df_original.isnull().values.any():\n",
    "    print('open shape before: ', open_df_original.shape)\n",
    "    open_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('open shape after: ', open_df_original.shape)    \n",
    "if high_df_original.isnull().values.any():\n",
    "    print('high shape before: ', high_df_original.shape)\n",
    "    high_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', high_df_original.shape)  \n",
    "if low_df_original.isnull().values.any():\n",
    "    print('low shape before: ', low_df_original.shape)\n",
    "    low_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('low shape after: ', low_df_original.shape)    \n",
    "if adjclose_df_original.isnull().values.any():\n",
    "    print('high shape before: ', adjclose_df_original.shape)\n",
    "    adjclose_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', adjclose_df_original.shape)    \n",
    "if volume_df_original.isnull().values.any():\n",
    "    print('high shape before: ', volume_df_original.shape)\n",
    "    volume_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', volume_df_original.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hawaiian-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open price rows duplicated -->  1\n",
      "high price rows duplicated -->  1\n",
      "low price rows duplicated -->  1\n",
      "adjusted close price rows duplicated -->  1\n",
      "volume rows duplicated -->  1\n"
     ]
    }
   ],
   "source": [
    "# to see how many duplicated for open, we have to performe it in a better way\n",
    "print('open price rows duplicated --> ', open_df_original.duplicated().sum())\n",
    "print('high price rows duplicated --> ', high_df_original.duplicated().sum())\n",
    "print('low price rows duplicated --> ', low_df_original.duplicated().sum())\n",
    "print('adjusted close price rows duplicated --> ', adjclose_df_original.duplicated().sum())\n",
    "print('volume rows duplicated --> ', volume_df_original.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nuclear-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- OPEN PRICE --> Last row is duplicated !!\n",
      "- HIGH PRICE --> Last row is duplicated !!\n",
      "- LOW PRICE --> Last row is duplicated !!\n",
      "- ADJUSTED CLOSE PRICE --> Last row is duplicated !!\n",
      "- VOLUME PRICE --> Last row is duplicated !!\n"
     ]
    }
   ],
   "source": [
    "if (open_df_original.iloc[-1:].values == open_df_original.iloc[-2:-1].values).all():\n",
    "    print('- OPEN PRICE --> Last row is duplicated !!')\n",
    "    open_df = open_df_original.drop(index=open_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- OPEN PRICE --> It is not the last row which is duplicated !!')\n",
    "    \n",
    "if (high_df_original.iloc[-1:].values == high_df_original.iloc[-2:-1].values).all():\n",
    "    print('- HIGH PRICE --> Last row is duplicated !!')\n",
    "    high_df = high_df_original.drop(index=high_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- HIGH PRICE --> It is not the last row which is duplicated !!')\n",
    "    \n",
    "if (low_df_original.iloc[-1:].values == low_df_original.iloc[-2:-1].values).all():\n",
    "    print('- LOW PRICE --> Last row is duplicated !!')\n",
    "    low_df = low_df_original.drop(index=low_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- LOW PRICE --> It is not the last row which is duplicated !!')   \n",
    "    \n",
    "if (adjclose_df_original.iloc[-1:].values == adjclose_df_original.iloc[-2:-1].values).all():\n",
    "    print('- ADJUSTED CLOSE PRICE --> Last row is duplicated !!')\n",
    "    adjclose_df = adjclose_df_original.drop(index=adjclose_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- ADJUSTED CLOSE PRICE --> It is not the last row which is duplicated !!')   \n",
    "    \n",
    "if (volume_df_original.iloc[-1:].values == volume_df_original.iloc[-2:-1].values).all():\n",
    "    print('- VOLUME PRICE --> Last row is duplicated !!')\n",
    "    volume_df = volume_df_original.drop(index=volume_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- VOLUME PRICE --> It is not the last row which is duplicated !!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "maritime-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potresti mettere il keep last tipo\n",
    "open_df.drop(index='2021-03-04', inplace=True)\n",
    "high_df.drop(index='2021-03-04', inplace=True)\n",
    "low_df.drop(index='2021-03-04', inplace=True)\n",
    "adjclose_df.drop(index='2021-03-04', inplace=True)\n",
    "volume_df.drop(index='2021-03-04', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-county",
   "metadata": {},
   "source": [
    "# 2. Second part: computation and assessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-recording",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * SKLearn preprocessing import to scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "conscious-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-technique",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Relative variation from open to adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "convinced-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative variation from open to adjusted close price\n",
    "adjclose_rel_var_df = (adjclose_df-open_df)/open_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-serum",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Absolute variation between high and low price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "african-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute variation between high and low price\n",
    "high_low_var_df = (high_df-low_df)\n",
    "\n",
    "# to scale the absolute variation between min&max value\n",
    "high_low_var_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "high_low_var_scaled = high_low_var_scaler.fit_transform(high_low_var_df)\n",
    "high_low_var_scaled_df = pd.DataFrame(data=high_low_var_scaled, index=high_low_var_df.index, columns=high_low_var_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-observer",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% High low absolute variation over adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "historical-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high low absolute variation over adjusted close price\n",
    "high_low_var_df_adjclose = high_low_var_df/adjclose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-singer",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Log Return (adjusted close price log return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "assisted-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjclose_df_log_return = np.log(adjclose_df/adjclose_df.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-better",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Construction of stack dataset with all features and label to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "agricultural-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>adjclose_rel_var</th>\n",
       "      <th>high_low_var_scaled</th>\n",
       "      <th>high_low_var_adjclose</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>-0.034963</td>\n",
       "      <td>0.500741</td>\n",
       "      <td>0.060920</td>\n",
       "      <td>68.286049</td>\n",
       "      <td>4985900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>-0.017036</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>62.909668</td>\n",
       "      <td>5055000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.737778</td>\n",
       "      <td>0.083912</td>\n",
       "      <td>68.643150</td>\n",
       "      <td>3261200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.086302</td>\n",
       "      <td>63.613949</td>\n",
       "      <td>4742800.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.860741</td>\n",
       "      <td>0.094314</td>\n",
       "      <td>69.873184</td>\n",
       "      <td>3341800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZYXI</th>\n",
       "      <th>2021-02-24</th>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.113978</td>\n",
       "      <td>0.045060</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>316300.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <td>-0.054540</td>\n",
       "      <td>0.163441</td>\n",
       "      <td>0.060734</td>\n",
       "      <td>17.453199</td>\n",
       "      <td>153936.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <td>-0.092500</td>\n",
       "      <td>0.404301</td>\n",
       "      <td>0.150138</td>\n",
       "      <td>14.520000</td>\n",
       "      <td>2341500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>-0.051034</td>\n",
       "      <td>0.146236</td>\n",
       "      <td>0.066712</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>1243500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>-0.033243</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>883800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139472 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 adjclose_rel_var  high_low_var_scaled  high_low_var_adjclose  \\\n",
       "     date                                                                       \n",
       "A    2020-03-11         -0.034963             0.500741               0.060920   \n",
       "     2020-03-12         -0.017036             0.622222               0.079161   \n",
       "     2020-03-13          0.031762             0.737778               0.083912   \n",
       "     2020-03-16          0.003216             0.697778               0.086302   \n",
       "     2020-03-17          0.058364             0.860741               0.094314   \n",
       "...                           ...                  ...                    ...   \n",
       "ZYXI 2021-02-24          0.022198             0.113978               0.045060   \n",
       "     2021-02-25         -0.054540             0.163441               0.060734   \n",
       "     2021-02-26         -0.092500             0.404301               0.150138   \n",
       "     2021-03-01         -0.051034             0.146236               0.066712   \n",
       "     2021-03-02         -0.033243             0.090323               0.050526   \n",
       "\n",
       "                  adjclose     volume  label  \n",
       "     date                                     \n",
       "A    2020-03-11  68.286049  4985900.0    0.0  \n",
       "     2020-03-12  62.909668  5055000.0    1.0  \n",
       "     2020-03-13  68.643150  3261200.0    0.0  \n",
       "     2020-03-16  63.613949  4742800.0    1.0  \n",
       "     2020-03-17  69.873184  3341800.0    0.0  \n",
       "...                    ...        ...    ...  \n",
       "ZYXI 2021-02-24  18.420000   316300.0    0.0  \n",
       "     2021-02-25  17.453199   153936.0    0.0  \n",
       "     2021-02-26  14.520000  2341500.0    1.0  \n",
       "     2021-03-01  14.690000  1243500.0    0.0  \n",
       "     2021-03-02  14.250000   883800.0    0.0  \n",
       "\n",
       "[1139472 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_rel_var_df_t = adjclose_rel_var_df.transpose()\n",
    "adjclose_rel_var_df_stack = adjclose_rel_var_df_t.stack(dropna=False)\n",
    "\n",
    "high_low_var_scaled_df_t = high_low_var_scaled_df.transpose()\n",
    "high_low_var_scaled_df_stack = high_low_var_scaled_df_t.stack(dropna=False)\n",
    "\n",
    "high_low_var_df_adjclose_t = high_low_var_df_adjclose.transpose()\n",
    "high_low_var_df_adjclose_stack = high_low_var_df_adjclose_t.stack(dropna=False)\n",
    "\n",
    "adjclose_df_t = adjclose_df.transpose()\n",
    "adjclose_df_stack = adjclose_df_t.stack(dropna=False)\n",
    "\n",
    "volume_df_t = volume_df.transpose()\n",
    "volume_df_stack = volume_df_t.stack(dropna=False)\n",
    "\n",
    "# classification variable \n",
    "classification_df = (adjclose_df_log_return > 0) * 1\n",
    "classification_df_t = classification_df.transpose()\n",
    "classification_df_t.shift(axis = 1, periods = -1) # sign the day before a rally up with 1 and vice versa with 0 \n",
    "classification_df_stack = classification_df_t.shift(axis = 1, periods = -1).stack(dropna=False)\n",
    "\n",
    "data = {'adjclose_rel_var': adjclose_rel_var_df_stack, \n",
    "        'high_low_var_scaled': high_low_var_scaled_df_stack,\n",
    "        'high_low_var_adjclose': high_low_var_df_adjclose_stack,\n",
    "        'adjclose': adjclose_df_stack,\n",
    "        'volume': volume_df_stack,\n",
    "        'label': classification_df_stack}  \n",
    "\n",
    "df_concat = pd.concat(data, axis=1)\n",
    "df_concat.dropna(axis = 0, how = 'any', inplace=True)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-ecuador",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Check the correct number of row in the previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sporting-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the correct number of row in the previous dataframe\n",
    "len(open_df.columns) * len(open_df.index) - len(open_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-wednesday",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Construction of stack dataset with other features and label to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sorted-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>70.760002</td>\n",
       "      <td>71.730003</td>\n",
       "      <td>67.570000</td>\n",
       "      <td>68.286049</td>\n",
       "      <td>4985900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>67.139999</td>\n",
       "      <td>62.160000</td>\n",
       "      <td>62.909668</td>\n",
       "      <td>5055000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>66.529999</td>\n",
       "      <td>69.510002</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>68.643150</td>\n",
       "      <td>3261200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>63.410000</td>\n",
       "      <td>66.620003</td>\n",
       "      <td>61.130001</td>\n",
       "      <td>63.613949</td>\n",
       "      <td>4742800.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>66.019997</td>\n",
       "      <td>70.529999</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>69.873184</td>\n",
       "      <td>3341800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZYXI</th>\n",
       "      <th>2021-02-24</th>\n",
       "      <td>18.020000</td>\n",
       "      <td>18.580000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>316300.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.453199</td>\n",
       "      <td>153936.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>14.020000</td>\n",
       "      <td>14.520000</td>\n",
       "      <td>2341500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>1243500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>14.740000</td>\n",
       "      <td>14.930000</td>\n",
       "      <td>14.210000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>883800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139472 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      open       high        low   adjclose     volume  label\n",
       "     date                                                                    \n",
       "A    2020-03-11  70.760002  71.730003  67.570000  68.286049  4985900.0    0.0\n",
       "     2020-03-12  64.000000  67.139999  62.160000  62.909668  5055000.0    1.0\n",
       "     2020-03-13  66.529999  69.510002  63.750000  68.643150  3261200.0    0.0\n",
       "     2020-03-16  63.410000  66.620003  61.130001  63.613949  4742800.0    1.0\n",
       "     2020-03-17  66.019997  70.529999  63.939999  69.873184  3341800.0    0.0\n",
       "...                    ...        ...        ...        ...        ...    ...\n",
       "ZYXI 2021-02-24  18.020000  18.580000  17.750000  18.420000   316300.0    0.0\n",
       "     2021-02-25  18.459999  18.500000  17.440001  17.453199   153936.0    0.0\n",
       "     2021-02-26  16.000000  16.200001  14.020000  14.520000  2341500.0    1.0\n",
       "     2021-03-01  15.480000  15.480000  14.500000  14.690000  1243500.0    0.0\n",
       "     2021-03-02  14.740000  14.930000  14.210000  14.250000   883800.0    0.0\n",
       "\n",
       "[1139472 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_df_t = open_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "open_df_stack = open_df_t.stack()\n",
    "\n",
    "high_df_t = high_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "high_df_stack = high_df_t.stack()\n",
    "\n",
    "low_df_t = low_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "low_df_stack = low_df_t.stack()\n",
    "\n",
    "volume_df_t = volume_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "volume_df_stack = volume_df_t.stack()\n",
    "\n",
    "data = {'open': open_df_stack, \n",
    "        'high': high_df_stack,\n",
    "        'low': low_df_stack,\n",
    "        'adjclose': adjclose_df_stack,\n",
    "        'volume': volume_df_stack,\n",
    "        'label': classification_df_stack} # taken from before computation\n",
    "\n",
    "df_concat2 = pd.concat(data, axis = 1)\n",
    "df_concat2.dropna(axis = 0, how = 'any', inplace = True)\n",
    "df_concat2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-racing",
   "metadata": {},
   "source": [
    "##### Remember: .std() is sample deviation, whereas the standardization thru StandardScaler use the standard deviation (the difference is the denominator, in the sample std it is used N-1). Moreover, to compute the standard deviation, you can do it in this way: .std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-directive",
   "metadata": {},
   "source": [
    "# 3. Third part: logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "empirical-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-argument",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FUNCTION% Function for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stupid-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg(features_df, classification_array, train_size = 0.80, shuffle_value = True, stratify_value = None, random_state=None):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(features_df, classification_array, train_size = train_size, shuffle = shuffle_value, stratify = stratify_value, random_state = random_state)\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    lr.fit(x_train, y_train)\n",
    "\n",
    "    lr_pred = lr.predict(x_test)\n",
    "\n",
    "    # target_names = ['class 0', 'class 1']\n",
    "    # confusion_matrix(y_test, lr_pred, labels=target_names)\n",
    "    \n",
    "    return(confusion_matrix(y_test, lr_pred), classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-enhancement",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Definition of the StockClass dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suitable-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a dictionary to store stock as StockClass instances and for each stock get attribute\n",
    "# (take a look at _2_0_stock_dataframe_class.py for more information)\n",
    "stock_object_dictionary = {'{0}'.format(ticker): StockClass(ticker=ticker_list) for ticker in ticker_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-background",
   "metadata": {},
   "source": [
    "### a. Trial 1: No Shuffle (i.e. No Stratify) - 80/20 - scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "english-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling data\n",
    "        df_sc_scaled = StandardScaler().fit_transform(features_df)\n",
    "\n",
    "        shuffle_value = False # no shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = None\n",
    "        train_size = 0.80\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix, stock_object_dictionary['{0}'.format(ticker)].accuracy_report = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-complexity",
   "metadata": {},
   "source": [
    "### b. Trial 2: Shuffle&Stratify - 80/20 - scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "integrated-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling data\n",
    "        df_sc_scaled = StandardScaler().fit_transform(features_df)\n",
    "\n",
    "        shuffle_value = True # shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = classification_array # stratify fashion \n",
    "        train_size = 0.80\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix2, stock_object_dictionary['{0}'.format(ticker)].accuracy_report2 = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "reduced-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================\n",
      "================   Confusion Matrix   ================\n",
      "======================================================\n",
      "\n",
      "None\n",
      "\n",
      "======================================================\n",
      "=====================   Report   =====================\n",
      "======================================================\n",
      "\n",
      "None\n",
      "\n",
      "======================================================\n",
      "================   Confusion Matrix   ================\n",
      "======================================================\n",
      "\n",
      "None\n",
      "\n",
      "======================================================\n",
      "=====================   Report   =====================\n",
      "======================================================\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stock = 'AAPL'\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report)\n",
    "\n",
    "\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix2)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-bullet",
   "metadata": {},
   "source": [
    "### c. Trial 3: No Shuffle (i.e. No Stratify) - 70/30 - scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-charlotte",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling data\n",
    "        df_sc_scaled = StandardScaler().fit_transform(features_df)\n",
    "\n",
    "        shuffle_value = False # no shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = None\n",
    "        train_size = 0.70\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix, stock_object_dictionary['{0}'.format(ticker)].accuracy_report = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-conjunction",
   "metadata": {},
   "source": [
    "### d. Trial 4: Shuffle&Stratify - 70/30 - scaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling data\n",
    "        df_sc_scaled = StandardScaler().fit_transform(features_df)\n",
    "\n",
    "        shuffle_value = True # shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = classification_array # stratify fashion \n",
    "        train_size = 0.70\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix2, stock_object_dictionary['{0}'.format(ticker)].accuracy_report2 = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = 'AAPL'\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report)\n",
    "\n",
    "\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix2)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-induction",
   "metadata": {},
   "source": [
    "### e. Trial 5: No Shuffle (i.e. No Stratify) - 70/30 - no scaled features (apart Volume with MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop('label', axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling Volume\n",
    "        col_name = ['volume']\n",
    "        feature = df_concat.loc[ticker][col_name]\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(feature.values)\n",
    "        vol = scaler.transform(feature.values)\n",
    "        df_sc_scaled = features_df \n",
    "        df_sc_scaled[col_name] = vol\n",
    "\n",
    "        shuffle_value = False # no shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = None\n",
    "        train_size = 0.70\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix, stock_object_dictionary['{0}'.format(ticker)].accuracy_report = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-skill",
   "metadata": {},
   "source": [
    "### f. Trial 6: Shuffle&Stratify - 70/30 - no scaled features (apart Volume with MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "    try:\n",
    "        # features dataframe, without classification column\n",
    "        features_df = df_concat.loc[ticker]\n",
    "        features_df.drop(['label', ], axis=1, inplace=True)\n",
    "        \n",
    "        # classification array\n",
    "        classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "        # sacaling Volume\n",
    "        col_name = ['volume']\n",
    "        feature = df_concat.loc[ticker][col_name]\n",
    "        scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(feature.values)\n",
    "        vol = scaler.transform(feature.values)\n",
    "        df_sc_scaled = features_df \n",
    "        df_sc_scaled[col_name] = vol     \n",
    "        \n",
    "        shuffle_value = True # shuffle, because of this we cannot stratify our label\n",
    "        stratify_value = classification_array # stratify fashion \n",
    "        train_size = 0.70\n",
    "        random_state = None\n",
    "\n",
    "        stock_object_dictionary['{0}'.format(ticker)].confusion_matrix2, stock_object_dictionary['{0}'.format(ticker)].accuracy_report2 = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_state) \n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = 'AAPL'\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report)\n",
    "\n",
    "\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('================   Confusion Matrix   ================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# confusion matrix\n",
    "print(stock_object_dictionary['{0}'.format(stock)].confusion_matrix2)\n",
    "\n",
    "print('')\n",
    "print('======================================================')\n",
    "print('=====================   Report   =====================')\n",
    "print('======================================================')\n",
    "print('')\n",
    "\n",
    "# classification report\n",
    "print(stock_object_dictionary['{0}'.format(stock)].accuracy_report2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py_37] *",
   "language": "python",
   "name": "conda-env-py_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

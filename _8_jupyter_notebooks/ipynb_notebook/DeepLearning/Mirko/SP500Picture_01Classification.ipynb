{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "substantial-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_version 3.7.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-chair",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %CLASS% StockClass: used to create the stock dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lasting-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockClass(object):\n",
    "\n",
    "    # Initialization of the StockClass object with the ticker symbol which is use to construct a yf.Ticker object\n",
    "    def __init__(self, ticker, isin=None, exchangeid=None, sector=None, industry=None, country=None, pe=None, eps=None,\n",
    "                 insiderown=None, shsout=None, shsfloat=None, mktcap=None, income=None, sales=None,\n",
    "                 booksh=None, pb=None, roa=None, tp=None, roe=None, roi=None, employees=None, debteq=None, \n",
    "                 groupby=None, confusion_matrix=None, accuracy_report=None, \n",
    "                 confusion_matrix2=None, accuracy_report2=None):\n",
    "        self.name = ticker\n",
    "        self.isin = isin\n",
    "        self.exchangeid = exchangeid\n",
    "        # self.history is the method of the SotckClass object to store data in DataFrame format\n",
    "        self.history = pd.DataFrame\n",
    "        self.investing = pd.DataFrame\n",
    "        self.pickle = pd.DataFrame\n",
    "        self.not_found = np.array([['date', 'ticker']])\n",
    "        self.nanDiv = False\n",
    "        self.nanSplit = False\n",
    "        self.sector = None\n",
    "        self.industry = industry\n",
    "        self.country = country\n",
    "        self.pe = pe\n",
    "        self.eps = eps\n",
    "        self.insiderown = insiderown\n",
    "        self.shsout = shsout\n",
    "        self.shsfloat = shsfloat\n",
    "        self.mktcap = mktcap\n",
    "        self.income = income\n",
    "        self.sales = sales\n",
    "        self.bookh = booksh\n",
    "        self.pb = pb\n",
    "        self.roa = roa\n",
    "        self.tp = tp\n",
    "        self.roe = roe\n",
    "        self.roi = roi\n",
    "        self.employees = employees\n",
    "        self.debteq = debteq\n",
    "        self.rsi = pd.DataFrame\n",
    "        self.groupby = groupby\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.accuracy_report = accuracy_report\n",
    "        self.confusion_matrix2 = confusion_matrix2\n",
    "        self.accuracy_report2 = accuracy_report2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-compilation",
   "metadata": {},
   "source": [
    "# 1. First part: import data, cleaning and arranging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-performer",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Main packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-vertical",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-seafood",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welcome-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_df_original = pd.read_csv('open_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "high_df_original = pd.read_csv('high_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "low_df_original = pd.read_csv('low_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "adjclose_df_original = pd.read_csv('adjclose_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "volume_df_original = pd.read_csv('volume_2020-03-11_2021-03-11_4635', index_col='date')\n",
    "ticker_list = list(adjclose_df_original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "naughty-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop columns with the end of the series NaN (probably not quoted anymore)\n",
    "if open_df_original.isnull().values.any():\n",
    "    print('open shape before: ', open_df_original.shape)\n",
    "    open_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('open shape after: ', open_df_original.shape)    \n",
    "if high_df_original.isnull().values.any():\n",
    "    print('high shape before: ', high_df_original.shape)\n",
    "    high_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', high_df_original.shape)  \n",
    "if low_df_original.isnull().values.any():\n",
    "    print('low shape before: ', low_df_original.shape)\n",
    "    low_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('low shape after: ', low_df_original.shape)    \n",
    "if adjclose_df_original.isnull().values.any():\n",
    "    print('high shape before: ', adjclose_df_original.shape)\n",
    "    adjclose_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', adjclose_df_original.shape)    \n",
    "if volume_df_original.isnull().values.any():\n",
    "    print('high shape before: ', volume_df_original.shape)\n",
    "    volume_df_original.dropna(axis=1, how='any', inplace=True)\n",
    "    print('high shape after: ', volume_df_original.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brave-custom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open price rows duplicated -->  1\n",
      "high price rows duplicated -->  1\n",
      "low price rows duplicated -->  1\n",
      "adjusted close price rows duplicated -->  1\n",
      "volume rows duplicated -->  1\n"
     ]
    }
   ],
   "source": [
    "# to see how many duplicated for open, we have to performe it in a better way\n",
    "print('open price rows duplicated --> ', open_df_original.duplicated().sum())\n",
    "print('high price rows duplicated --> ', high_df_original.duplicated().sum())\n",
    "print('low price rows duplicated --> ', low_df_original.duplicated().sum())\n",
    "print('adjusted close price rows duplicated --> ', adjclose_df_original.duplicated().sum())\n",
    "print('volume rows duplicated --> ', volume_df_original.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "danish-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- OPEN PRICE --> Last row is duplicated !!\n",
      "- HIGH PRICE --> Last row is duplicated !!\n",
      "- LOW PRICE --> Last row is duplicated !!\n",
      "- ADJUSTED CLOSE PRICE --> Last row is duplicated !!\n",
      "- VOLUME PRICE --> Last row is duplicated !!\n"
     ]
    }
   ],
   "source": [
    "if (open_df_original.iloc[-1:].values == open_df_original.iloc[-2:-1].values).all():\n",
    "    print('- OPEN PRICE --> Last row is duplicated !!')\n",
    "    open_df = open_df_original.drop(index=open_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- OPEN PRICE --> It is not the last row which is duplicated !!')\n",
    "    \n",
    "if (high_df_original.iloc[-1:].values == high_df_original.iloc[-2:-1].values).all():\n",
    "    print('- HIGH PRICE --> Last row is duplicated !!')\n",
    "    high_df = high_df_original.drop(index=high_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- HIGH PRICE --> It is not the last row which is duplicated !!')\n",
    "    \n",
    "if (low_df_original.iloc[-1:].values == low_df_original.iloc[-2:-1].values).all():\n",
    "    print('- LOW PRICE --> Last row is duplicated !!')\n",
    "    low_df = low_df_original.drop(index=low_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- LOW PRICE --> It is not the last row which is duplicated !!')   \n",
    "    \n",
    "if (adjclose_df_original.iloc[-1:].values == adjclose_df_original.iloc[-2:-1].values).all():\n",
    "    print('- ADJUSTED CLOSE PRICE --> Last row is duplicated !!')\n",
    "    adjclose_df = adjclose_df_original.drop(index=adjclose_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- ADJUSTED CLOSE PRICE --> It is not the last row which is duplicated !!')   \n",
    "    \n",
    "if (volume_df_original.iloc[-1:].values == volume_df_original.iloc[-2:-1].values).all():\n",
    "    print('- VOLUME PRICE --> Last row is duplicated !!')\n",
    "    volume_df = volume_df_original.drop(index=volume_df_original.iloc[-1:].index)\n",
    "else:\n",
    "    print('- VOLUME PRICE --> It is not the last row which is duplicated !!')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clear-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potresti mettere il keep last tipo\n",
    "open_df.drop(index='2021-03-04', inplace=True)\n",
    "high_df.drop(index='2021-03-04', inplace=True)\n",
    "low_df.drop(index='2021-03-04', inplace=True)\n",
    "adjclose_df.drop(index='2021-03-04', inplace=True)\n",
    "volume_df.drop(index='2021-03-04', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-queensland",
   "metadata": {},
   "source": [
    "# 2. Second part: computation and assessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-mechanism",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * SKLearn preprocessing import to scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "diverse-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-origin",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Relative variation from open to adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "victorian-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative variation from open to adjusted close price\n",
    "adjclose_rel_var_df = (adjclose_df-open_df)/open_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-quantum",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Absolute variation between high and low price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "public-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# absolute variation between high and low price\n",
    "high_low_var_df = (high_df-low_df)\n",
    "\n",
    "# to scale the absolute variation between min&max value\n",
    "high_low_var_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "high_low_var_scaled = high_low_var_scaler.fit_transform(high_low_var_df)\n",
    "high_low_var_scaled_df = pd.DataFrame(data=high_low_var_scaled, index=high_low_var_df.index, columns=high_low_var_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-browser",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% High low absolute variation over adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bearing-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high low absolute variation over adjusted close price\n",
    "high_low_var_df_adjclose = high_low_var_df/adjclose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-foundation",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FEATURE% Log Return (adjusted close price log return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "standing-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjclose_df_log_return = np.log(adjclose_df/adjclose_df.shift(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-calculator",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Construction of stack dataset with all features and label to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swedish-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>adjclose_rel_var</th>\n",
       "      <th>high_low_var_scaled</th>\n",
       "      <th>high_low_var_adjclose</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>-0.034963</td>\n",
       "      <td>0.500741</td>\n",
       "      <td>0.060920</td>\n",
       "      <td>68.286049</td>\n",
       "      <td>4985900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>-0.017036</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.079161</td>\n",
       "      <td>62.909668</td>\n",
       "      <td>5055000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.737778</td>\n",
       "      <td>0.083912</td>\n",
       "      <td>68.643150</td>\n",
       "      <td>3261200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>0.003216</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.086302</td>\n",
       "      <td>63.613949</td>\n",
       "      <td>4742800.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>0.058364</td>\n",
       "      <td>0.860741</td>\n",
       "      <td>0.094314</td>\n",
       "      <td>69.873184</td>\n",
       "      <td>3341800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZYXI</th>\n",
       "      <th>2021-02-24</th>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.113978</td>\n",
       "      <td>0.045060</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>316300.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <td>-0.054540</td>\n",
       "      <td>0.163441</td>\n",
       "      <td>0.060734</td>\n",
       "      <td>17.453199</td>\n",
       "      <td>153936.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <td>-0.092500</td>\n",
       "      <td>0.404301</td>\n",
       "      <td>0.150138</td>\n",
       "      <td>14.520000</td>\n",
       "      <td>2341500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>-0.051034</td>\n",
       "      <td>0.146236</td>\n",
       "      <td>0.066712</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>1243500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>-0.033243</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>0.050526</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>883800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139472 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 adjclose_rel_var  high_low_var_scaled  high_low_var_adjclose  \\\n",
       "     date                                                                       \n",
       "A    2020-03-11         -0.034963             0.500741               0.060920   \n",
       "     2020-03-12         -0.017036             0.622222               0.079161   \n",
       "     2020-03-13          0.031762             0.737778               0.083912   \n",
       "     2020-03-16          0.003216             0.697778               0.086302   \n",
       "     2020-03-17          0.058364             0.860741               0.094314   \n",
       "...                           ...                  ...                    ...   \n",
       "ZYXI 2021-02-24          0.022198             0.113978               0.045060   \n",
       "     2021-02-25         -0.054540             0.163441               0.060734   \n",
       "     2021-02-26         -0.092500             0.404301               0.150138   \n",
       "     2021-03-01         -0.051034             0.146236               0.066712   \n",
       "     2021-03-02         -0.033243             0.090323               0.050526   \n",
       "\n",
       "                  adjclose     volume  label  \n",
       "     date                                     \n",
       "A    2020-03-11  68.286049  4985900.0    0.0  \n",
       "     2020-03-12  62.909668  5055000.0    1.0  \n",
       "     2020-03-13  68.643150  3261200.0    0.0  \n",
       "     2020-03-16  63.613949  4742800.0    1.0  \n",
       "     2020-03-17  69.873184  3341800.0    0.0  \n",
       "...                    ...        ...    ...  \n",
       "ZYXI 2021-02-24  18.420000   316300.0    0.0  \n",
       "     2021-02-25  17.453199   153936.0    0.0  \n",
       "     2021-02-26  14.520000  2341500.0    1.0  \n",
       "     2021-03-01  14.690000  1243500.0    0.0  \n",
       "     2021-03-02  14.250000   883800.0    0.0  \n",
       "\n",
       "[1139472 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjclose_rel_var_df_t = adjclose_rel_var_df.transpose()\n",
    "adjclose_rel_var_df_stack = adjclose_rel_var_df_t.stack(dropna=False)\n",
    "\n",
    "high_low_var_scaled_df_t = high_low_var_scaled_df.transpose()\n",
    "high_low_var_scaled_df_stack = high_low_var_scaled_df_t.stack(dropna=False)\n",
    "\n",
    "high_low_var_df_adjclose_t = high_low_var_df_adjclose.transpose()\n",
    "high_low_var_df_adjclose_stack = high_low_var_df_adjclose_t.stack(dropna=False)\n",
    "\n",
    "adjclose_df_t = adjclose_df.transpose()\n",
    "adjclose_df_stack = adjclose_df_t.stack(dropna=False)\n",
    "\n",
    "volume_df_t = volume_df.transpose()\n",
    "volume_df_stack = volume_df_t.stack(dropna=False)\n",
    "\n",
    "# classification variable \n",
    "classification_df = (adjclose_df_log_return > 0) * 1\n",
    "classification_df_t = classification_df.transpose()\n",
    "classification_df_t.shift(axis = 1, periods = -1) # sign the day before a rally up with 1 and vice versa with 0 \n",
    "classification_df_stack = classification_df_t.shift(axis = 1, periods = -1).stack(dropna=False)\n",
    "\n",
    "data = {'adjclose_rel_var': adjclose_rel_var_df_stack, \n",
    "        'high_low_var_scaled': high_low_var_scaled_df_stack,\n",
    "        'high_low_var_adjclose': high_low_var_df_adjclose_stack,\n",
    "        'adjclose': adjclose_df_stack,\n",
    "        'volume': volume_df_stack,\n",
    "        'label': classification_df_stack}  \n",
    "\n",
    "df_concat = pd.concat(data, axis=1)\n",
    "df_concat.dropna(axis = 0, how = 'any', inplace=True)\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-assessment",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Check the correct number of row in the previous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fancy-journalism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139472"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the correct number of row in the previous dataframe\n",
    "len(open_df.columns) * len(open_df.index) - len(open_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-bacon",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Construction of stack dataset with other features and label to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "impressed-documentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>70.760002</td>\n",
       "      <td>71.730003</td>\n",
       "      <td>67.570000</td>\n",
       "      <td>68.286049</td>\n",
       "      <td>4985900.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>67.139999</td>\n",
       "      <td>62.160000</td>\n",
       "      <td>62.909668</td>\n",
       "      <td>5055000.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>66.529999</td>\n",
       "      <td>69.510002</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>68.643150</td>\n",
       "      <td>3261200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>63.410000</td>\n",
       "      <td>66.620003</td>\n",
       "      <td>61.130001</td>\n",
       "      <td>63.613949</td>\n",
       "      <td>4742800.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>66.019997</td>\n",
       "      <td>70.529999</td>\n",
       "      <td>63.939999</td>\n",
       "      <td>69.873184</td>\n",
       "      <td>3341800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZYXI</th>\n",
       "      <th>2021-02-24</th>\n",
       "      <td>18.020000</td>\n",
       "      <td>18.580000</td>\n",
       "      <td>17.750000</td>\n",
       "      <td>18.420000</td>\n",
       "      <td>316300.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-25</th>\n",
       "      <td>18.459999</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>17.453199</td>\n",
       "      <td>153936.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-26</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.200001</td>\n",
       "      <td>14.020000</td>\n",
       "      <td>14.520000</td>\n",
       "      <td>2341500.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>15.480000</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>14.690000</td>\n",
       "      <td>1243500.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-02</th>\n",
       "      <td>14.740000</td>\n",
       "      <td>14.930000</td>\n",
       "      <td>14.210000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>883800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1139472 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      open       high        low   adjclose     volume  label\n",
       "     date                                                                    \n",
       "A    2020-03-11  70.760002  71.730003  67.570000  68.286049  4985900.0    0.0\n",
       "     2020-03-12  64.000000  67.139999  62.160000  62.909668  5055000.0    1.0\n",
       "     2020-03-13  66.529999  69.510002  63.750000  68.643150  3261200.0    0.0\n",
       "     2020-03-16  63.410000  66.620003  61.130001  63.613949  4742800.0    1.0\n",
       "     2020-03-17  66.019997  70.529999  63.939999  69.873184  3341800.0    0.0\n",
       "...                    ...        ...        ...        ...        ...    ...\n",
       "ZYXI 2021-02-24  18.020000  18.580000  17.750000  18.420000   316300.0    0.0\n",
       "     2021-02-25  18.459999  18.500000  17.440001  17.453199   153936.0    0.0\n",
       "     2021-02-26  16.000000  16.200001  14.020000  14.520000  2341500.0    1.0\n",
       "     2021-03-01  15.480000  15.480000  14.500000  14.690000  1243500.0    0.0\n",
       "     2021-03-02  14.740000  14.930000  14.210000  14.250000   883800.0    0.0\n",
       "\n",
       "[1139472 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_df_t = open_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "open_df_stack = open_df_t.stack()\n",
    "\n",
    "high_df_t = high_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "high_df_stack = high_df_t.stack()\n",
    "\n",
    "low_df_t = low_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "low_df_stack = low_df_t.stack()\n",
    "\n",
    "volume_df_t = volume_df.loc[adjclose_df_log_return.index[0]:adjclose_df_log_return.index[-1]].transpose()\n",
    "volume_df_stack = volume_df_t.stack()\n",
    "\n",
    "data = {'open': open_df_stack, \n",
    "        'high': high_df_stack,\n",
    "        'low': low_df_stack,\n",
    "        'adjclose': adjclose_df_stack,\n",
    "        'volume': volume_df_stack,\n",
    "        'label': classification_df_stack} # taken from before computation\n",
    "\n",
    "df_concat2 = pd.concat(data, axis = 1)\n",
    "df_concat2.dropna(axis = 0, how = 'any', inplace = True)\n",
    "df_concat2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-relaxation",
   "metadata": {
    "tags": []
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp; * Definition of the StockClass dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pressing-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of a dictionary to store stock as StockClass instances and for each stock get attribute\n",
    "# (take a look at _2_0_stock_dataframe_class.py for more information)\n",
    "stock_object_dictionary = {'{0}'.format(ticker): StockClass(ticker=ticker_list) for ticker in ticker_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-shadow",
   "metadata": {},
   "source": [
    "##### Remember: .std() is sample deviation, whereas the standardization thru StandardScaler use the standard deviation (the difference is the denominator, in the sample std it is used N-1). Moreover, to compute the standard deviation, you can do it in this way: .std(ddof=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-bowling",
   "metadata": {},
   "source": [
    "# 3. Third part: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "virgin-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lightweight-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-thanksgiving",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp; %FUNCTION% Function for calling back DL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "concerned-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_limit=.7\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (logs.get('accuracy')>accuracy_limit):\n",
    "            print('--- accuracy higher than ', accuracy_limit)\n",
    "            self.model.stop_training=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "quality-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "\n",
    "# features dataframe, without classification column\n",
    "features_df = df_concat.loc[ticker]\n",
    "features_df.drop('label', axis=1, inplace=True)\n",
    "\n",
    "# classification array\n",
    "classification_array = df_concat.loc[ticker]['label'].values\n",
    "\n",
    "# sacaling data\n",
    "df_sc_scaled = StandardScaler().fit_transform(features_df)\n",
    "\n",
    "shuffle_value = False # no shuffle, because of this we cannot stratify our label\n",
    "stratify_value = None\n",
    "train_size = 0.80\n",
    "random_state = None\n",
    "\n",
    "# define train set and test set \n",
    "x_train, x_test, y_train, y_test = train_test_split(features_df, classification_array, train_size = train_size, shuffle = shuffle_value, stratify = stratify_value, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "electric-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "nervous-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.Dense(units=500, activation=tf.nn.relu),\n",
    "                                   tf.keras.layers.Dense(units=2, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "assisted-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "parallel-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 5045256.3438 - accuracy: 0.4959\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1087718.7500 - accuracy: 0.5223\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 615851.8320 - accuracy: 0.4583\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 320830.0586 - accuracy: 0.4604\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 721615.3281 - accuracy: 0.4835\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 806444.4062 - accuracy: 0.5810\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 648528.6875 - accuracy: 0.4947\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 865422.8203 - accuracy: 0.4271\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1141503.7500 - accuracy: 0.5597\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 950378.8516 - accuracy: 0.4933\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 506599.6133 - accuracy: 0.4608\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 973545.8906 - accuracy: 0.5505\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 472619.5273 - accuracy: 0.4287\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 330738.5938 - accuracy: 0.4882\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 707935.0234 - accuracy: 0.4868\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 443803.2109 - accuracy: 0.4574\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 834us/step - loss: 118160.6104 - accuracy: 0.5684\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 211295.0273 - accuracy: 0.5299\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 449132.2188 - accuracy: 0.4252\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 682355.7578 - accuracy: 0.4677\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 398194.5781 - accuracy: 0.4712\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 224136.2734 - accuracy: 0.5195\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 225861.8916 - accuracy: 0.5683\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 656873.1758 - accuracy: 0.4311\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 463992.2891 - accuracy: 0.4738\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 437850.9961 - accuracy: 0.5340\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 413890.3008 - accuracy: 0.5530\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 116159.2617 - accuracy: 0.5787\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 885151.0078 - accuracy: 0.4254\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1526652.3281 - accuracy: 0.5393\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1029279.7812 - accuracy: 0.4554\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1027795.4062 - accuracy: 0.5892\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 668us/step - loss: 665240.2734 - accuracy: 0.5268\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 574076.1602 - accuracy: 0.5147\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 321652.1963 - accuracy: 0.5203\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 143077.8306 - accuracy: 0.5073\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 685239.0938 - accuracy: 0.4833\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1014269.3125 - accuracy: 0.5551\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 384520.9766 - accuracy: 0.5369\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 781370.0273 - accuracy: 0.5371\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 520633.4961 - accuracy: 0.4970\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 379069.9258 - accuracy: 0.5463\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 495329.0664 - accuracy: 0.4626\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 407533.8164 - accuracy: 0.4612\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 424022.5098 - accuracy: 0.5203\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 484102.6797 - accuracy: 0.5800\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 568583.6348 - accuracy: 0.4413\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1601754.5781 - accuracy: 0.4444\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 480788.6328 - accuracy: 0.5435\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 172284.5854 - accuracy: 0.5013\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 442131.1250 - accuracy: 0.4595\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 951309.6016 - accuracy: 0.4355\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 606360.6367 - accuracy: 0.4930\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 395132.0156 - accuracy: 0.5861\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 756728.4453 - accuracy: 0.5120\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 423732.3730 - accuracy: 0.5426\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 258508.7285 - accuracy: 0.5124\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 158810.9004 - accuracy: 0.5732\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 150649.6162 - accuracy: 0.5082\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 177974.4121 - accuracy: 0.5202\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 95175.3877 - accuracy: 0.4892\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 416820.0911 - accuracy: 0.5118\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 807499.8281 - accuracy: 0.4343\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 907095.8125 - accuracy: 0.5519\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 632415.1562 - accuracy: 0.4846\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 136136.6865 - accuracy: 0.4657\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 219922.7969 - accuracy: 0.5514\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 223449.7598 - accuracy: 0.5245\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 376952.5352 - accuracy: 0.4014\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 407436.5273 - accuracy: 0.4381\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 712073.1641 - accuracy: 0.5781\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 503279.0703 - accuracy: 0.4615\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 175525.0449 - accuracy: 0.5425\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 79844.6499 - accuracy: 0.5571\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 75258.7617 - accuracy: 0.5241\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 114136.2344 - accuracy: 0.5421\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 115071.0381 - accuracy: 0.5323\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 202136.7031 - accuracy: 0.4452\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 126819.4609 - accuracy: 0.5257\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 200165.4375 - accuracy: 0.5219\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 300964.0273 - accuracy: 0.4691\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 430674.1914 - accuracy: 0.4170\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 371721.9688 - accuracy: 0.5033\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 211009.2402 - accuracy: 0.4782\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 298607.4973 - accuracy: 0.4831\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 781020.0391 - accuracy: 0.5951\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 1667787.4531 - accuracy: 0.4296\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 898631.0195 - accuracy: 0.5889\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 428553.1211 - accuracy: 0.4548\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 281692.3711 - accuracy: 0.5104\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 309453.9980 - accuracy: 0.5023\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 501us/step - loss: 499227.3711 - accuracy: 0.5293\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 566788.9102 - accuracy: 0.5715\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 300506.0020 - accuracy: 0.4171\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 427184.6211 - accuracy: 0.4831\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 446005.2500 - accuracy: 0.4848\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 409987.5352 - accuracy: 0.5497\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 500us/step - loss: 185571.6211 - accuracy: 0.4770\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 181321.9175 - accuracy: 0.5719\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 667us/step - loss: 398859.7109 - accuracy: 0.4656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a516f16648>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_object_dictionary['{0}'.format(ticker)].confusion_matrix, stock_object_dictionary['{0}'.format(ticker)].accuracy_report = logistic_reg(df_sc_scaled, classification_array, train_size, shuffle_value, stratify_value, random_stat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-february",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-physics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-destruction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "burning-settle",
   "metadata": {},
   "source": [
    "### a. Trial 1: No Shuffle (i.e. No Stratify) - 80/20 - scaled features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py_37] *",
   "language": "python",
   "name": "conda-env-py_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

Index: analysis/network_monitor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Imported packages\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom datetime import date, timedelta\r\nimport time\r\nimport networkx as nx\r\nfrom collections import Counter\r\n\r\n# Project's imports\r\nfrom api_moduls.stock_dataframe_class import StockListClass\r\nfrom pickle_obj.read_pickle_df import read_available_ticker_from_pickle, read_not_found_from_pickle, \\\r\n    read_data_for_analysis_linearInterp_for_nan\r\n# from analysis_functions.correlation import cross_correlation_function\r\n\r\n\r\ndef network_monitor_function():\r\n    start_date = (date.today() - timedelta(180)).strftime('%Y-%m-%d')\r\n    end_date = date.today().strftime('%Y-%m-%d')\r\n    stock_object_dictionary = read_data_for_analysis_linearInterp_for_nan(max_not_found_record=5, start=start_date,\r\n                                                                          end=end_date)\r\n    ticker_list = []\r\n    for element in stock_object_dictionary:\r\n        ticker_list.append(element)\r\n\r\n    # construction of the open prices\r\n    # open_df = pd.DataFrame([stock_object_dictionary['{0}'.format(element)].history['Open'] for element in\r\n    #                         stock_object_dictionary.copy()], index=list(stock_object_dictionary.keys())).transpose()\r\n\r\n    # construction of the high prices\r\n    # high_df = pd.DataFrame([stock_object_dictionary['{0}'.format(element)].history['High'] for element in\r\n    #                         stock_object_dictionary.copy()], index=list(stock_object_dictionary.keys())).transpose()\r\n\r\n    # construction of the low prices\r\n    # low_df = pd.DataFrame([stock_object_dictionary['{0}'.format(element)].history['Low'] for element in\r\n    #                         stock_object_dictionary.copy()], index=list(stock_object_dictionary.keys())).transpose()\r\n\r\n    # construction of the adjusted close prices\r\n    adjclose_df = pd.DataFrame([stock_object_dictionary['{0}'.format(element)].history['Adj Close'] for element in\r\n                                stock_object_dictionary.copy()], index=list(stock_object_dictionary.keys())).transpose().astype('float32')\r\n\r\n    # manage nan value and take off (in the read module) those series with too many nan, or with observation that are not at least the 60% of the total analysis period\r\n    corr_matrix_adjclose = np.corrcoef(adjclose_df.transpose().to_numpy()).astype('float32')    # int64arr = ones((1024, 1024), dtype=np.uint64)\r\n    # corr_matrix_adjclose = adjclose_df.corr(method='pearson').astype('float32')\r\n    upper_trin_corr_matrix = np.triu(corr_matrix_adjclose, 1).astype('float32')\r\n    upper_trin_corr_matrix = np.where(upper_trin_corr_matrix <= 0.000001, np.nan, upper_trin_corr_matrix) # mettere intervallo intorno a zero se si vogliono tenere le incorrelate\r\n    quantile = np.nanquantile(upper_trin_corr_matrix, 0.75).astype('float32')\r\n    corr_matrix_adjclose = np.where(corr_matrix_adjclose <= quantile, 0, corr_matrix_adjclose)\r\n    corr_matrix_adjclose = np.triu(corr_matrix_adjclose, 1).astype('float32') # keeping just the upper triangle matrix\r\n\r\n    # np.savetxt(\"test2CorrMatrix.csv\", corr_matrix_adjclose, delimiter=\",\")\r\n    # correlation method: pearson, kendall, spearman and callable\r\n    # corr_matrix_adjclose = adjclose_df.corr(method='pearson').astype('float16')\r\n\r\n    '''numpy.core._exceptions.MemoryError: Unable to allocate 197. MiB for an array with shape (1, 25786084) and data type float64'''\r\n\r\n    del stock_object_dictionary\r\n\r\n    corr_matrix_adjclose_df = pd.DataFrame(corr_matrix_adjclose, index=[i for i in ticker_list], columns=[i for i in ticker_list], dtype=np.float32)\r\n    corr_matrix_adjclose_df = corr_matrix_adjclose_df.where(np.triu(np.ones(corr_matrix_adjclose_df.shape), 1).astype(np.bool))\r\n    corr_matrix_adjclose_df = corr_matrix_adjclose_df.stack().reset_index()\r\n    corr_matrix_adjclose_df.columns = ['Source', 'Target', 'Weight']\r\n    corr_matrix_adjclose_df = corr_matrix_adjclose_df[corr_matrix_adjclose_df['Weight'] != 0]\r\n    corr_matrix_adjclose_df.reset_index(drop=True, inplace=True)\r\n    zipper = list(zip(corr_matrix_adjclose_df['Source'], corr_matrix_adjclose_df['Target']))\r\n    graph = nx.Graph((x, y, {'weight': corr_matrix_adjclose_df.Weight[zipper.index((x, y))]}) for (x, y), v in\r\n                       Counter(zipper).items())\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    for (x, y), v in Counter(zipper).items():\r\n        graph.add_edge(x, y, weight=corr_matrix_adjclose_df.Weight[zipper.index((x, y))])\r\n\r\n    graph = nx.DiGraph((x, y, {'weight': corr_matrix_adjclose_df.Weight[zipper.index((x, y))]}) for (x, y), v in Counter(zipper).items())\r\n\r\n    g = nx.DiGraph((x, y, {'weight': df.Value[Ed.index((x, y))]}) for (x, y), v in Counter(Ed).items())\r\n\r\n\r\n\r\n\r\n\r\n    df = pd.DataFrame(np.where(np.triu(np.ones(corr_matrix_adjclose.shape), 1).astype(np.bool)))\r\n    df2 = pd.DataFrame(df, columns=[\"A\", \"B\"])\r\n\r\n    G = nx.from_numpy_matrix(corr_matrix_adjclose)\r\n\r\n    quantile = corr_matrix_adjclose.melt().value.quantile(0.75).astype('float16')\r\n\r\n    corr_matrix_adjclose.apply(lambda x: [y if y >= quantile else 0 for y in x])\r\n    # numpy compute corr in horizontal, it is necessary to convert the transposed dataframe (example adjclose_df.transpose())\r\n    # to numpy (from dataframe to array) and then compute the correlation matrix as num = np.corrcoef(adjclose_df.transpose())\r\n    # trin = np.triu(num, 1) prendo solamente il triangolo sopra la diagonale e il resto lo metto a zero\r\n    # np.where(trin <= 0.000001, np.nan, trin) sostituisco tutti i valori di trin che sono prossimi a zerocon nan\r\n    # quantile = np.nanquantile(ok, 0.75)  calcolo il quantile che voglio escludendo i nan\r\n    # np.where(num <= quantile, 0, num) metto tutti le correlazioni inferiori alla soglia determinata pari a zero e cosÃ¬\r\n    # la mia matrice di correlazione diventa un' adjacency matrix\r\n\r\n\r\n     # poi costruire il network con \r\n\r\n    ######## Not useful anymore np.fill_diagonal(num2, np.nan)\r\n    # quantile np.quantile(num, 0.75)\r\n    # np.fill_diagonal(num2, np.nan)\r\n    print(corr_matrix_adjclose)\r\n\r\n    '''\r\n    #############################################################################\r\n    ###   corr_matrix_adjclose.describe().transpose() # to get Series stats   ###\r\n    #############################################################################\r\n    '''\r\n\r\n    column_headers_list = corr_matrix_adjclose.columns.values.tolist()\r\n\r\n    # TODO: la prossima riga dovrebbe impiantarsi e dare errore di memoria prova a bloccare l'esecuzione alla\r\n    #  prossima riga e  lanciarla dal Terminal, nel caso non dovesse darti problemi, prova ad eseguire anche la riga successiva\r\n    #  sempre dal terminal ... itera questa operazione fino a riscontrare un problema di memoria\r\n\r\n    # corr_matrix_one_lag = np.zeros((len(column_headers_list), len(column_headers_list)))\r\n\r\n    # corr_matrix_two_lag = np.zeros((len(column_headers_list), len(column_headers_list)))\r\n\r\n    # Computation of log return of each stock over adjusted close prices\r\n    # if you want there is also the method --> df['pct_change'] = df.price.pct_change()\r\n    # and it compute percentage change\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    log_return_adjclose_df = pd.DataFrame()\r\n    for i, col in enumerate(column_headers_list):\r\n        log_return_adjclose_df[col] = np.log(adjclose_df[col]) - np.log(adjclose_df[col].shift(-1))\r\n\r\n    \"\"\"\r\n    ## Corr Adjustment ##\r\n    # input of a rule to choose the most fittable correlation to the model (in this case it is chosen the .7 quant)\r\n    # to erase those correlation that do not achive the chosen threshold\r\n    \"\"\"\r\n\r\n    del stock_object_dictionary\r\n\r\n    tempo = time.time()\r\n    corr_matrix_lag = pd.DataFrame()\r\n    i = 0\r\n    for element in column_headers_list:\r\n        df = log_return_adjclose_df\r\n        df[element] = df[element].shift(1)\r\n        corr_matrix_lag = corr_matrix_lag.append(df.corr()[element])\r\n        print(\"row: \", i, \"--- %s seconds ---\" % (tempo - time.time()))\r\n        i += 1\r\n\r\n    print('arrivati')\r\n\r\n    lag = 1\r\n\r\n    # we need a faster method to compute the cross-correlation matrix of 5000 stocks\r\n\r\n    for i, col in enumerate(column_headers_list):\r\n        for j, col in enumerate(column_headers_list):\r\n            if i != j:\r\n                # to measure the effect of i on j\r\n                corr_matrix_one_lag[i][j] = \\\r\n                    log_return_adjclose_df[column_headers_list[j]].corr(\r\n                        log_return_adjclose_df[column_headers_list[i]].shift(lag))\r\n\r\n            if (i % 50) == 0 and (j % 50) == 0: print('row: ', i, ', column: ', j,\r\n                                                      \" --- %s seconds ---\" % (tempo - time.time()))\r\n    # https://riptutorial.com/pandas/example/9812/using-hdfstore\r\n    # https://stackoverflow.com/questions/33171413/cross-correlation-time-lag-correlation-with-pandas/55490747\r\n    print('arrivati')\r\n\r\n    # it is done in this way because the library networkx does (Notes: it is the opposite way of the theory of the book \"Networks: An Introduction\")\r\n\r\n    # then insert the efficient performance on the lagged i.e. try different lag periods\r\n    ### for col in db_adj.columns:\r\n    ###     xycorr = [crosscorr(db_adj[col], db_adj[col], lag=i) for i in range(12)] # call function for lagged correlation between two Series\r\n    ###\r\n\r\n    print(corr_matrix)\r\n\r\n\r\ndef main():\r\n    network_monitor_function()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n# # Imported packages\r\n# import os\r\n# import numpy as np\r\n# import pandas as pd\r\n# from QuantLib import *\r\n# import datetime\r\n#\r\n# # Network package\r\n# import networkx as nx\r\n#\r\n# # Project's imports\r\n# from pickle_obj.read_pickle_df import read_data_for_analysis\r\n# from moduls.stock_dataframe_class import StockClass, AnalysisDatesClass\r\n\r\n\r\nfrom datetime import date, timedelta\r\n\r\n\r\n# import matplotlib.pyplot as plt\r\n#     import networkx as nx\r\n#     import pandas as pd\r\n#     import yfinance as yf\r\n# import yfinance as yf\r\n# from datetime import date\r\n# import tkinter as tk\r\n# from tkinter import filedialog\r\n#\r\n# from datetime import *\r\n# from QuantLib import *\r\n#\r\n# import matplotlib.pyplot  as plt\r\n\r\n\r\n# Beginning of the period_app script\r\n# if __name__ == \"__main__\":\r\n#     main()\r\n\r\n\r\ndef network_function(stock_object_dictionary, ticker_list_object, important_dates):\r\n    # construction of the headers and sub-headers of the big DF of all stocks and their market information\r\n    header = [np.array(ticker_list_object.ticker_list),\r\n              np.array(len(ticker_list_object.ticker_list) * ['Date_Time', 'OpenPrice', 'HighPrice',\r\n                                                              'LowPrice', 'ClosePrice', 'AdjClose',\r\n                                                              'Volume', 'Dividends', 'StockSplits'])]\r\n    # setting of the number of observations used for the analysis\r\n    obs = str(200)  # number of observations to use for the analysis\r\n    # setting of the sub-headers array\r\n    sub_headers = np.array(['OpenPrice', 'HighPrice', 'LowPrice', 'ClosePrice',\r\n                            'AdjClose', 'Volume', 'Dividends', 'StockSplits'])\r\n\r\n    # setting of the trading day\r\n    # right now it set with the day observations of the first stock in the ticker_list_object i.e. American Express\r\n    days = stock_object_dictionary['AXP_object'].history.index\r\n\r\n    header = [np.array(['TimeStamp']), np.array(['Date_Time'])]\r\n    db = pd.DataFrame(days, index=days, columns=header)\r\n\r\n    # for cycle to clean dataset in Sql\r\n    for i, ticker in enumerate(ticker_list_object.ticker_list):\r\n        # construction of the header and sub-headers of the big Table/Database\r\n        header = [np.array(len(sub_headers) * [ticker.lower()]), sub_headers]\r\n        # converting the stock_object_dictionary history DataFrame into an array\r\n        historical_series = np.array(stock_object_dictionary['{0}_object'.format(ticker)].history)\r\n        # creating an adjusted df for the fetched serie to attach to the db\r\n        adj_historical_series = pd.DataFrame(historical_series, index=days, columns=header)\r\n        db = db.join(adj_historical_series)  # construction of the final database\r\n    db = db.drop('TimeStamp', axis=1)\r\n    project_root = os.path.dirname(os.path.dirname(__file__))\r\n    db_path = project_root + '\\static\\data\\dbFianle10days.xlsx'\r\n    db.to_excel(db_path, index=True, header=True)\r\n\r\n    ##############################################################################\r\n    ### Vedi se riesci a fare una funzione per tutti questi passaggi ############\r\n    ##############################################################################\r\n    ### soprattutto prova a fare correlazione e segui l'andamento della stock  ###\r\n    ############# dalla chiusura precedente all'apertura seguente ################\r\n    ##############################################################################\r\n\r\n    db_adj = db.filter(like='AdjClose')\r\n\r\n    db_adj.columns = db_adj.columns.droplevel(-1)  # to drop the underlevel header\r\n\r\n    db_adj = db_adj.apply(\r\n        pd.to_numeric)  # to convert from object to float64 (numeric)...Otherwise I cannot apply corr function\r\n    # oppure DataFrame.convert_dtypes(infer_objects=True, convert_string=True, convert_integer=True, convert_boolean=True)\r\n\r\n    corr_db_adj = db_adj.corr(method='pearson')\r\n\r\n    # corr_db_adj.describe().transpose() # for Series stats\r\n\r\n    colheaderslist = db_adj.columns.values.tolist()\r\n\r\n    corr_matrix = np.zeros((len(colheaderslist), len(colheaderslist)))\r\n\r\n    # computation of log return of each stock\r\n    # if you want there is also the method --> df['pct_change'] = df.price.pct_change()\r\n    db_adj_lnReturndf = pd.DataFrame()\r\n    for i, col in enumerate(colheaderslist):\r\n        db_adj_lnReturndf[col] = np.log(db_adj[col]) - np.log(db_adj[col].shift(-1))\r\n\r\n    \"\"\"\r\n    ## Corr Adjustment ##\r\n    # input of a rule to choose the most fittable correlation to the model (in this case it is chosen the .7 quant)\r\n    # to erase those correlation that do not achive the chosen threshold\r\n    \"\"\"\r\n    for i, col in enumerate(colheaderslist):\r\n        for j, col in enumerate(colheaderslist):\r\n            if i != j:\r\n                corr_matrix[i][j] = crosscorr(db_adj_lnReturndf[colheaderslist[i]],\r\n                                              db_adj_lnReturndf[colheaderslist[j]],\r\n                                              lag=1)  # to measure the effect of i on j\r\n                # it is done in this way because the library networkx does (Notes: it is the opposite way of the theory of the book \"Networks: An Introduction\")\r\n\r\n                # then insert the efficient performance on the lagged i.e. try different lag periods\r\n                ### for col in db_adj.columns:\r\n                ###     xycorr = [crosscorr(db_adj[col], db_adj[col], lag=i) for i in range(12)] # call function for lagged correlation between two Series\r\n                ###\r\n\r\n    print(corr_matrix)\r\n\r\n    ticker_list_modified = list(map(str.lower, [ele.replace('.', '_') for ele in\r\n                                                ticker_list]))  # it save the list of all retrieved tickers in lower case and\r\n    # it replaces \".\" with \"_\"\r\n\r\n    ddbb = pd.DataFrame(corr_matrix)  # convert corr_matrix (array) to dataframe type\r\n    ddbb.index = ticker_list_modified  # to rename indices of the dataframe\r\n    ddbb.columns = ticker_list_modified  # to rename columns of the dataframe\r\n\r\n    \"\"\"\r\n    BEGINING TO BUILD THE GRAPH (NETWORK)  \r\n    \"\"\"\r\n\r\n    # https://networkx.github.io/documentation/stable/auto_examples/drawing/plot_directed.html\r\n\r\n    ##for col in db_adj.columns:\r\n    # xycorr = [crosscorr(db_adj[col], db_adj[col], lag=i) for i in range(12)] # call function for lagged correlation between two Series\r\n    ###\r\n    ##for col in dfdf.columns:\r\n    ##    print(dfdf[col].iloc[:])\r\n    # per creare il network dalla adjacency matrix\r\n    ##A = np.array([[1, 1], [2, 1]])\r\n    ##G = nx.from_numpy_matrix(A)\r\n    ###\r\n\r\n    # db['a2a_mi','OpenPrice']['2020-09-11'] # to access single elements\r\n\r\n    # idx = pd.IndexSlice\r\n    # db.to_excel(r'dbFinale2.xlsx', index = True)\r\n    # db['a2a_mi','OpenPrice'].loc[idx[days[2]]]\r\n\r\n    # ################### to access data into the dataframe #####################\r\n\r\n    # df = pd.DataFrame({ 'Date': rng, 'Val': np.random.randn(len(rng)) })\r\n    # finalDF22 = DateDFdf.join(adjserie)\r\n    # idx = pd.IndexSlice\r\n\r\n    # df['location','S1'].loc[idx['a']]\r\n\r\n    # df.loc(axis=1)[:,'S1']\r\n    # df['location','S1']\r\n\r\n    # graph = nx.karate_club_graph()\r\n\r\n    # plt.figure(figsize =(30, 30))\r\n    # nx.draw_networkx(graph, with_labels = True)\r\n\r\n    # writer1 = pd.ExcelWriter('nodes.xlsx', engine='xlsxwriter')\r\n    # dfnodes = pd.DataFrame(graph.nodes())\r\n\r\n    # dfnodes.to_excel(writer1, sheet_name='Foglio1')\r\n\r\n    # writer2 = pd.ExcelWriter('edges.xlsx', engine='xlsxwriter')\r\n    # dfedges = pd.DataFrame(graph.edges())\r\n\r\n    # dfedges.to_excel(writer2, sheet_name='Foglio1')\r\n\r\n    # writer1.save()\r\n    # writer2.save()\r\n\r\n    # bet_centrality = nx.betweenness_centrality(graph, normalized = True,\r\n    #                                               endpoints = False)\r\n\r\n    # close_centrality = nx.closeness_centrality(graph)\r\n\r\n    # deg_centrality = nx.degree_centrality(graph)\r\n\r\n    # # #define the ticker symbol\r\n    # tickerSymbol = 'BET.MI'\r\n\r\n    # # #get data on this ticker\r\n    # tickerData = yf.Ticker(tickerSymbol)\r\n\r\n    # # #get the historical prices for this ticker\r\n    # tickerDf = tickerData.history(period='1d', start='2010-1-1', end='2020-5-31') #â1dâ (daily), â1moâ (monthly), â1yâ (yearly)\r\n\r\n    # # #see your data that is a Pandas dataframe\r\n    # tickerDf\r\n\r\n    # # #define the ticker symbol\r\n    # # tickerSymbol = 'BET.MI'\r\n\r\n    # # #get data on this ticker\r\n    # # tickerData = yf.Ticker(tickerSymbol)\r\n\r\n    # # #info on the company\r\n    # tickerData.info\r\n\r\n    # # #https://towardsdatascience.com/how-to-get-stock-data-using-python-c0de1df17e75\r\n\r\n# def main():\r\n#     # Setting of Start date and End date of our retrieving period\r\n#     arctic = ArcticDatesClass(start='2020-11-01', end='2020-12-01')\r\n#     stock_object_dictionary, ticker_list_object = arctic_retrieve_function(start_date=arctic.start_date, end_date=arctic.end_date)\r\n#     date_entry1 = input('Enter the start date for the analysis in YYYY-MM-DD format')\r\n#     date_entry2 = input('Enter the end date for the analysis in YYYY-MM-DD format')\r\n#     year1, month1, day1 = map(int, date_entry1.split('-'))\r\n#     year2, month2, day2 = map(int, date_entry2.split('-'))\r\n#     analysis = AnalysisDatesClass(start=datetime.datetime(year1,month1, day1), end=datetime.datetime(year2,month2, day2))\r\n#\r\n#     # Call of the network analysis function\r\n#     network_function(stock_object_dictionary, ticker_list_object, analysis)\r\n#\r\n#\r\n# # Beginning of the period_app script\r\n# if __name__ == \"__main__\":\r\n#     main()\r\n#\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- analysis/network_monitor.py	(revision 5a012b2e24dcb8bdadc6948868eca27af1cf0887)
+++ analysis/network_monitor.py	(date 1611154610259)
@@ -1,16 +1,13 @@
 # Imported packages
 import numpy as np
 import pandas as pd
-from datetime import date, timedelta
 import time
 import networkx as nx
 from collections import Counter
+from datetime import date, timedelta
 
 # Project's imports
-from api_moduls.stock_dataframe_class import StockListClass
-from pickle_obj.read_pickle_df import read_available_ticker_from_pickle, read_not_found_from_pickle, \
-    read_data_for_analysis_linearInterp_for_nan
-# from analysis_functions.correlation import cross_correlation_function
+from pickle_obj.read_pickle_df import read_data_for_analysis_linearInterp_for_nan
 
 
 def network_monitor_function():
@@ -45,7 +42,6 @@
     upper_trin_corr_matrix = np.where(upper_trin_corr_matrix <= 0.000001, np.nan, upper_trin_corr_matrix) # mettere intervallo intorno a zero se si vogliono tenere le incorrelate
     quantile = np.nanquantile(upper_trin_corr_matrix, 0.75).astype('float32')
     corr_matrix_adjclose = np.where(corr_matrix_adjclose <= quantile, 0, corr_matrix_adjclose)
-    corr_matrix_adjclose = np.triu(corr_matrix_adjclose, 1).astype('float32') # keeping just the upper triangle matrix
 
     # np.savetxt("test2CorrMatrix.csv", corr_matrix_adjclose, delimiter=",")
     # correlation method: pearson, kendall, spearman and callable
@@ -54,8 +50,13 @@
     '''numpy.core._exceptions.MemoryError: Unable to allocate 197. MiB for an array with shape (1, 25786084) and data type float64'''
 
     del stock_object_dictionary
+    del upper_trin_corr_matrix
+    del quantile
 
     corr_matrix_adjclose_df = pd.DataFrame(corr_matrix_adjclose, index=[i for i in ticker_list], columns=[i for i in ticker_list], dtype=np.float32)
+
+    del corr_matrix_adjclose
+
     corr_matrix_adjclose_df = corr_matrix_adjclose_df.where(np.triu(np.ones(corr_matrix_adjclose_df.shape), 1).astype(np.bool))
     corr_matrix_adjclose_df = corr_matrix_adjclose_df.stack().reset_index()
     corr_matrix_adjclose_df.columns = ['Source', 'Target', 'Weight']
